{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/STRMNGnwo/CT5133-DeepLearning/blob/main/DeepLearningAssignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CT5133 - Assignment 1\n",
        "\n",
        "  -Submitted by Srinivas Ilancheran (19280039) and Lukasz Szemet (19502109) of the MSc Artificial Intelligence programme\n",
        "\n"
      ],
      "metadata": {
        "id": "8kPCghdty9Sv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Sections:"
      ],
      "metadata": {
        "id": "0FJ88ToL0g6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1"
      ],
      "metadata": {
        "id": "HztzM9Oj02Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions:**\n",
        "\n",
        "Implement Logistic Regression (Topic 2):\n",
        "  1. Use Jupyter Notebook (Python or R) to implement a neural network approach to logistic regression (no hidden layers, one output node)\n",
        "  2. Your code should follow my notes to implement the algorithm from scratch.\n",
        "  3. Your notebook should include a brief description of the algorithm, with all references.\n",
        "  4. Your code must handle different numbers of inputs and different numbers of training cases, but you don't have to support more than one binary output node"
      ],
      "metadata": {
        "id": "1lL8cTmG1DH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1 notes\n",
        "\n",
        "Class node\n",
        "4 functions\n",
        "contructor(input then init weights), activation(sigmoid function), weighted sum(w .  x + b), forward\n"
      ],
      "metadata": {
        "id": "LDUkdOnxdI7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UT8Xtr5QiG2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionNeuron:\n",
        "\n",
        "  def __init__(self,data_shape):\n",
        "\n",
        "\n",
        "    self.weights=np.random.rand(*data_shape)\n",
        "\n",
        "    #self.weights=np.squeeze(self.weights)\n",
        "    '''the squeeze is done to change the shape from 1,3 to just 3, (Which is the shape of the input)\n",
        "    this is necessary as in the dot product function, we're doing input * weight (input is of shape 3,0 which would make dot product with 1,3 shape impossible)\n",
        "\n",
        "    An easier fix would simply be changing the order of dot product to be weight* input (1,3 can be multiplied with shape 3,0)\n",
        "\n",
        "    '''\n",
        "    print(\"Initial weights matrix:\", self.weights)\n",
        "    print(\"Shape of Initial weights matrix:\", self.weights.shape)\n",
        "    # initialising a bias value\n",
        "    self.bias=np.random.rand(1,1)\n",
        "\n",
        "\n",
        "  def weighted_sum_func(self,input):\n",
        "\n",
        "    return np.dot(self.weights,input) + self.bias\n",
        "\n",
        "\n",
        "  def activation_func(self,input):\n",
        "\n",
        "      return 1/(1+np.exp(-input))\n",
        "\n",
        "\n",
        "  def forward(self,input):\n",
        "\n",
        "    #defining a single pass\n",
        "\n",
        "    #send the input into the weighted sum function, to get W . input\n",
        "\n",
        "    weighted_sum=self.weighted_sum_func(input)\n",
        "\n",
        "    #send the weighted_sum into the activation function (sigmoid) to get either a 0, or a 1 (based on threshold value)\n",
        "    activation_output=self.activation_func(weighted_sum)\n",
        "\n",
        "    return activation_output\n",
        "\n"
      ],
      "metadata": {
        "id": "JQTSPzI6zNwm"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model,training_data_X,training_data_Y, training_params):\n",
        "\n",
        "  # SGD parameters\n",
        "  epochs=training_params[\"epochs\"]\n",
        "  max_iterations= len(training_data_X)*epochs\n",
        "\n",
        "  print(\"Max iterations: \",max_iterations)\n",
        "  learning_rate=training_params[\"lr\"]\n",
        "\n",
        "  #probability threshold:\n",
        "  threshold=0.5\n",
        "\n",
        "  samples=training_data_X.to_numpy()\n",
        "  labels=training_data_Y.to_numpy()\n",
        "\n",
        "  i=0\n",
        "  prev_loss=0\n",
        "  running_loss=0\n",
        "  #Training loop to perform SGD:\n",
        "  while i<max_iterations:\n",
        "    #randomly choose a training sample from samples\n",
        "    rand_sample_index=np.random.randint(len(samples))\n",
        "\n",
        "    input_sample=np.array(samples[rand_sample_index])\n",
        "\n",
        "    #print(\"Shape of input sample:\",input_sample.shape)\n",
        "\n",
        "    #input_sample -> for the should be of shape (1,num.attributes) as its a single row containing num.attributes columns\n",
        "\n",
        "    #model output -> probability\n",
        "    probability=model.forward(input_sample)\n",
        "\n",
        "    #convert probability into label\n",
        "    predicted_label=1 if probability>0.5 else 0\n",
        "\n",
        "    #calculate loss using Stochastic Gradient Descent\n",
        "    # input_sample[1] is true class value (Actually, a sample's attributes-> samples[rand_sample], true class value-> labels[rand_sample])\n",
        "    curr_loss = -((labels[rand_sample_index]*np.log(probability)) + (1-labels[rand_sample_index]) * np.log(1-probability))\n",
        "\n",
        "    delta_w = np.zeros(model.weights.shape)\n",
        "\n",
        "    for idx, weight in enumerate(model.weights):\n",
        "      delta_w[idx] = (probability - labels[rand_sample_index])*input_sample[idx]\n",
        "\n",
        "    delta_b = (probability - labels[rand_sample_index])\n",
        "\n",
        "    for idx, weight in enumerate(model.weights):\n",
        "      model.weights[idx] -= learning_rate * delta_w[idx]\n",
        "\n",
        "    model.bias -= learning_rate * delta_b\n",
        "\n",
        "    i += 1\n",
        "    running_loss += curr_loss\n",
        "\n",
        "    if i%len(samples)==0:\n",
        "      print(f\"{i/len(samples)} Epochs finished\")\n",
        "      if (running_loss-prev_loss) < 10**(-12): #latter condition is to check for convergence (change in weights is minimal)\n",
        "        print(f\"Converged! after {i} iterations \")\n",
        "        i = max_iterations\n",
        "      prev_loss = running_loss\n",
        "      running_loss = 0\n"
      ],
      "metadata": {
        "id": "d6f7u3EvmToR"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 2"
      ],
      "metadata": {
        "id": "Lwatvyh504vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instructions:**\n",
        "\n",
        "• I will supply two fairly small datasets:\n",
        "  – One will be linearly separable (almost or fully), the other will not\n",
        "  – I will provide sample Python code to load and plot the datasets; you are\n",
        "allowed to use this code in your own assignment\n",
        "\n",
        "• Divide each dataset randomly into:\n",
        "  – Training set (70%):use for main training\n",
        "  – Validation set (15%): use for tuning, e.g. selecting learning rates\n",
        "  – Test set (15%): held out set for final performance evaluation\n",
        "\n",
        "• Train a logistic regressor using your code from\n",
        "Part 1, and see how it performs on both datasets\n",
        "\n",
        "• In your notebook, summarise results and\n",
        "provide observations and conclusions\n"
      ],
      "metadata": {
        "id": "RaBW9cDV1wJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using pandas to read in the csv\n",
        "blobs_dataset=pd.read_csv(\"/content/blobs400.csv\")\n",
        "circles_dataset=pd.read_csv(\"/content/circles500.csv\")\n",
        "\n",
        "print(\"Blobs dataset:\")\n",
        "print(blobs_dataset.head())\n",
        "print(\"\\n\\nCircles dataset:\")\n",
        "print(circles_dataset.head())"
      ],
      "metadata": {
        "id": "P3Nba_KXoEm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50dad1fb-9f77-4b45-c5b6-3408cf773980"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blobs dataset:\n",
            "         X1        X2        X3  Class\n",
            "0  1.418221  2.124375 -0.433905      1\n",
            "1  1.590404  0.935434  1.510369      1\n",
            "2  2.311458 -1.026668  1.031930      1\n",
            "3  1.186782  0.591894  0.563649      1\n",
            "4  1.661888  4.047231  0.987049      0\n",
            "\n",
            "\n",
            "Circles dataset:\n",
            "         X0        X1  Class\n",
            "0  0.180647  0.552945      1\n",
            "1 -0.188674  0.325629      1\n",
            "2  0.413742  0.931251      0\n",
            "3 -0.199223  0.902665      0\n",
            "4  0.488279 -0.341202      1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_blobs = blobs_dataset.sample(frac=0.7, random_state=69)\n",
        "training_circles = circles_dataset.sample(frac=0.7, random_state=69)\n",
        "\n",
        "training_blobs = blobs_dataset.sample(frac=0.7, random_state=69)\n",
        "training_circles = circles_dataset.sample(frac=0.7, random_state=69)\n",
        "\n",
        "#dividing the attributes and the label in the training blobs training dataset\n",
        "training_blobs_X=training_blobs.loc[:, [\"X1\",\"X2\",\"X3\"]]\n",
        "training_blobs_Y= training_blobs.loc[:,[\"Class\"]]\n",
        "\n",
        "print(training_blobs.iloc[[0]].shape)\n",
        "\n",
        "print(training_blobs_X.shape)\n",
        "\n",
        "print(training_blobs_Y.shape)"
      ],
      "metadata": {
        "id": "hX9GJBw4s-4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c06e58d-2395-4118-d832-76311e72a8fd"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 4)\n",
            "(280, 3)\n",
            "(280, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initialising the LogisiticRegression Neuron\n",
        "LR_Model=LogisticRegressionNeuron(np.array(training_blobs_X.iloc[[0]]).shape)\n",
        "\n",
        "#setting training params\n",
        "train_params={ \"epochs\":10000,\"lr\":0.000001}\n",
        "\n",
        "#training the model on the blobs dataset's training partition\n",
        "train_model(LR_Model,training_blobs_X,training_blobs_Y, train_params)"
      ],
      "metadata": {
        "id": "GsCWBxL6sX0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281802b1-1c88-4816-82b8-97fa613265af"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial weights matrix: [[0.06984089 0.35381306 0.14937838]]\n",
            "Shape of Initial weights matrix: (1, 3)\n",
            "Max iterations:  2800000\n",
            "1.0 Epochs finished\n",
            "2.0 Epochs finished\n",
            "3.0 Epochs finished\n",
            "4.0 Epochs finished\n",
            "Converged! after 1120 iterations \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"New weights matrix value:\", LR_Model.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD73SBmI6YpQ",
        "outputId": "c0d0c322-033d-4d94-adb1-12ae4b56a914"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New weights matrix value: [[0.06886319 0.35283536 0.14840068]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model's accuracy on the datasets"
      ],
      "metadata": {
        "id": "qPHJNr5o8GEb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tfWdPWe68L2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 3"
      ],
      "metadata": {
        "id": "4z5Yawwu07Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 4"
      ],
      "metadata": {
        "id": "VFGBp94H08gK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 5"
      ],
      "metadata": {
        "id": "v7BQDYcC09tZ"
      }
    }
  ]
}